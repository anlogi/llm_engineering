{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: Generative AI can be used to generate high-quality content such as articles, social media posts, product descriptions, and even entire books. This can help businesses save time and resources while maintaining consistency in their content.\n",
      "2. **Marketing Automation**: Generative AI can be used to automate marketing processes such as lead generation, email campaigns, and personalized recommendations. For example, a company can use generative AI to generate customized product recommendations for customers based on their browsing history.\n",
      "3. **Product Design**: Generative AI can be used to design new products, such as 3D models, logos, and packaging designs. This can help businesses speed up the product development process while maintaining high-quality standards.\n",
      "4. **Customer Service Chatbots**: Generative AI can be used to power customer service chatbots that can respond to customer inquiries, provide support, and even resolve issues on their own.\n",
      "5. **Image Generation**: Generative AI can be used to generate high-quality images for various applications such as advertising, packaging, and product photography.\n",
      "6. **Data Analysis**: Generative AI can be used to analyze large datasets and identify patterns, trends, and insights that may not be visible through traditional analysis methods.\n",
      "7. **Predictive Maintenance**: Generative AI can be used to predict equipment failures and maintenance needs, reducing downtime and increasing efficiency in industries such as manufacturing and healthcare.\n",
      "8. **Personalization**: Generative AI can be used to personalize customer experiences across various channels such as email marketing, social media, and product recommendations.\n",
      "9. **Creative Writing**: Generative AI can be used to generate creative writing such as poetry, short stories, and even entire scripts for films or plays.\n",
      "10. **Business Intelligence**: Generative AI can be used to analyze large datasets and identify insights that can inform business decisions, such as predicting market trends or identifying new business opportunities.\n",
      "\n",
      "Some specific examples of businesses using generative AI include:\n",
      "\n",
      "* **Netflix**: Using generative AI to recommend TV shows and movies to customers based on their viewing history.\n",
      "* **Amazon**: Using generative AI to generate personalized product recommendations for customers.\n",
      "* **Walmart**: Using generative AI to optimize inventory management and reduce waste.\n",
      "* **McDonald's**: Using generative AI to create new menu items and improve customer experiences.\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As the technology continues to evolve, we can expect to see even more innovative uses in various industries.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: Generative AI can be used to generate high-quality content such as blog posts, social media posts, product descriptions, and even entire books. This can help businesses save time and resources while maintaining consistency in their content.\n",
      "2. **Marketing Automation**: Generative AI can be used to create personalized marketing campaigns, personalize emails, and even generate ad copy. This can help businesses increase their conversion rates and improve customer engagement.\n",
      "3. **Product Design**: Generative AI can be used to design new products, such as furniture, electronics, or even entire product lines. This can help businesses reduce the time and cost associated with traditional product design methods.\n",
      "4. **Chatbots and Virtual Assistants**: Generative AI can be used to create more advanced chatbots and virtual assistants that can understand natural language and provide personalized responses. This can help businesses improve their customer service and support operations.\n",
      "5. **Data Analysis and Visualization**: Generative AI can be used to analyze large datasets and generate visualizations such as charts, graphs, and heat maps. This can help businesses gain insights into their data and make more informed decisions.\n",
      "6. **Speech Recognition and Synthesis**: Generative AI can be used to improve speech recognition systems, allowing for more accurate transcription of audio recordings. Additionally, it can also be used to generate synthetic voices for voiceovers, podcasts, or even entire audiobooks.\n",
      "7. **Image Generation**: Generative AI can be used to generate high-quality images such as product photos, interior designs, and even artwork. This can help businesses save time and resources while maintaining consistency in their visual branding.\n",
      "8. **Music and Audio Production**: Generative AI can be used to create new music tracks, sound effects, or even entire audio albums. This can help businesses improve the quality of their audio content and reduce production costs.\n",
      "9. **Customer Service**: Generative AI can be used to create more advanced customer service systems that can understand natural language and provide personalized responses. This can help businesses improve their customer satisfaction and loyalty.\n",
      "10. **Predictive Maintenance**: Generative AI can be used to predict equipment failures, reducing downtime and increasing overall efficiency in industries such as manufacturing and energy.\n",
      "\n",
      "Some specific business applications of generative AI include:\n",
      "\n",
      "* **Amazon's Product Recommendations**: Amazon uses generative AI to recommend products to customers based on their browsing history and purchase behavior.\n",
      "* **Google's Image Search**: Google uses generative AI to improve its image search functionality, allowing users to search for images by description or keywords.\n",
      "* **Microsoft's Chatbots**: Microsoft uses generative AI to create chatbots that can understand natural language and provide personalized responses to customers.\n",
      "* **IBM's Watson**: IBM's Watson uses generative AI to analyze large datasets and provide insights and recommendations to businesses.\n",
      "\n",
      "These are just a few examples of the many business applications of generative AI. As the technology continues to evolve, we can expect to see even more innovative applications in the future.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI, also known as Generative Model or GANs (Generative Adversarial Networks), has a wide range of applications in various industries and businesses. Here are some examples of business applications of Generative AI:\n",
      "\n",
      "1. **Product Design and Development**: Generative AI can be used to design new products, such as furniture, electronics, or clothing. It can generate 2D or 3D designs, prototypes, and even entire product lines.\n",
      "2. **Content Creation**: Generating high-quality content such as images, videos, articles, and social media posts using AI algorithms can automate the content creation process for businesses. This can help reduce costs and increase efficiency.\n",
      "3. **Marketing and Advertising**: Generative AI can be used to create personalized advertisements and marketing campaigns based on customer data and preferences. It can also generate new product ideas and promotional materials.\n",
      "4. **Graphic Design and Visual Content Creation**: Generative AI can be used to create high-quality graphics, logos, and videos for businesses. It can also help with video editing and post-production.\n",
      "5. **Music and Audio Generation**: Generative AI models can be used to compose music, generate sound effects, and even create entire audio tracks.\n",
      "6. **Cybersecurity**: Generative AI can be used to identify and detect cyber threats by generating new attack scenarios and testing defenses in a virtual environment.\n",
      "7. **Data Analysis and Visualization**: Generative AI can be used to analyze and visualize large datasets, creating patterns and insights that would be difficult for humans to spot manually.\n",
      "8. **Language Translation**: Generative AI models can be used to translate languages more efficiently and accurately than traditional translation methods.\n",
      "9. **Customer Service Chatbots**: Generative AI can be used to create conversational chatbots that use natural language processing (NLP) to understand customer queries and provide personalized responses.\n",
      "10. **Supply Chain Optimization**: Generative AI can be used to optimize supply chains by predicting demand, forecasting inventory levels, and identifying bottlenecks in the logistics chain.\n",
      "\n",
      "Some of the key benefits of Generative AI for businesses include:\n",
      "\n",
      "* **Increased Efficiency**: Automating repetitive tasks and generating new content, designs, or ideas with minimal supervision.\n",
      "* **Improved Accuracy**: Reducing the risk of human error through automation and quality control checks.\n",
      "* **Cost Savings**: Reducing costs associated with manual labor, data entry, and other time-consuming tasks.\n",
      "* **Innovation and Creativity**: Generating new and innovative products, services, or marketing campaigns that can differentiate businesses from their competitors.\n",
      "\n",
      "However, Generative AI also has its limitations, such as:\n",
      "\n",
      "* **Bias and Discrimination**: The AI algorithm may perpetuate biases or discriminatory practices present in the training data.\n",
      "* **Job Displacement**: Automation of tasks may displace certain jobs or industries, particularly those requiring high levels of manual labor or routine work.\n",
      "\n",
      "To fully realize the potential of Generative AI, businesses should develop strategies to mitigate these limitations and ensure that their employees are equipped with the necessary skills to complement the automation.\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aabd4debf0c8: 100% ▕██████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling c5ad996bda6e: 100% ▕██████████████████▏  556 B                         \u001b[K\n",
      "pulling 6e4c38e1172f: 100% ▕██████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd: 100% ▕██████████████████▏  148 B                         \u001b[K\n",
      "pulling a85fe2a2e58e: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the definitions of a few key concepts behind Large Language Models (LLMs). The terms mentioned are neural networks, attention, and transformers. Let me break this down step by step.\n",
      "\n",
      "First, I know that an LLM is primarily made up of something called a \"neural network.\" But what exactly is this neural network? From what I've heard before, neural networks are designed to simulate the way the human brain works. They consist of layers of neurons, right? Each layer has units or nodes that receive inputs and produce some sort of activation signal.\n",
      "\n",
      "I think these neural networks are often deep and have multiple layers. Each layer could process different parts of the data differently. But how exactly do they work to make predictions or generate responses?\n",
      "\n",
      "Wait, I remember something about these models having a lot of parameters—like billions. That probably means there's a lot of processing power needed to train them effectively.\n",
      "\n",
      "Now, the second concept is attention. I've heard that attention plays a central role in understanding how models process information. What does attention really mean here? Maybe it refers to how the model determines what parts of its input are most relevant for generating an output. It's like paying attention to specific keywords or events in a text.\n",
      "\n",
      "I think attention can be implemented in different ways, though. For example, some models might focus on local patterns within smaller chunks of the data, while others use more global awareness across the entire dataset. This contextual sensitivity must be crucial for generating coherent responses from an LLM.\n",
      "\n",
      "Next up is the transformer architecture itself. Transformers seem like a recent big advancement in machine learning with respect to language models. They usually involve multiple layers where each layer processes information through self-attention mechanisms.\n",
      "\n",
      "What are these self-attention mechanisms? I think they allow each model unit to compute its own attention, which captures relationships between different parts of the input data. This could help in understanding dependencies and context within sequences.\n",
      "\n",
      "I also recall something about frequency analysis. Do transformers use frequency-based representations? That might include embeddings for words and their contexts. Using these global features could enhance the model's ability to understand sentence structures and other linguistic patterns.\n",
      "\n",
      "So, putting it all together: LLMs are neural networks with multiple layers that process information using attention mechanisms within a transformer framework. These systems learn from data by paying attention to certain parts of the input and generating outputs based on global context gained through attention and self-attention processes.\n",
      "</think>\n",
      "\n",
      "Large Language Models (LLMs), such as GPT, are composed of several key components that work together to generate human-like responses from text. Here's a structured explanation of their underlying concepts:\n",
      "\n",
      "1. **Neural Network Architecture**:\n",
      "   - A neural network is modeled after the structure of neurons in the brain, consisting of layers with interconnected nodes (units).\n",
      "   - Each layer processes data differently and often involves multiple hidden layers.\n",
      "   - These networks are typically deep and capable of processing large amounts of information simultaneously.\n",
      "\n",
      "2. **Attention Mechanism**:\n",
      "   - Attention determines the relevance of parts of the input or model's knowledge when generating an output.\n",
      "   - For LLMs, it identifies the most relevant words or events in a text document to produce coherent responses.\n",
      "   - This local focus aids in understanding context and generating more accurate answers.\n",
      "\n",
      "3. **Transformer Architecture**:\n",
      "   - Transformer models use multiple layers with self-attention mechanisms.\n",
      "   - In each layer, units process information based on their own attention, capturing relationships between different data parts.\n",
      "   - These layers enable the model to understand sentence structure by incorporating global features from embeddings of words and their contexts.\n",
      "\n",
      "4. **Global Context**:\n",
      "   - Transformers employ frequency-based representations, including embeddings for context and word embedding vectors.\n",
      "   - This helps in understanding linguistic patterns beyond local context, enhancing the model's ability to produce nuanced responses.\n",
      "\n",
      "In summary, while LLMs consist of neural networks with multiple layers and attention mechanisms (specifically self-attention), transformers employ even more sophisticated structure and layer operations. These components work together to enable models to understand language by paying attention to relevant parts and leveraging global contextual features.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ff0675-3e40-4131-aa76-81f971f8de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a6a739-d766-4bc7-81ea-aed606d9ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab35da3c-1c93-4767-b7e7-3deaaa6c9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49adb0a6-a702-44b7-86ee-4d83d88e03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "MODEL=\"llama3.2\"\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama.chat(model=MODEL, messages = messages_for(website))\n",
    "  \n",
    "    return response.message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee41df-99cd-4697-a416-7dd3db4d6c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d0f0e-8c9f-4014-8ddf-c32db9a40a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7af4e3-4695-4b16-a2be-d5afdf2c9755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Summary of Edward Donner\\'s Website\\n\\n## About the Creator and Company\\n\\nEdward Donner is a tech enthusiast, writer, and entrepreneur. He is the co-founder and CTO of **Nebula.io**, an AI startup that applies AI to help people discover their potential. Nebula.io works with groundbreaking LLMs (Large Language Models) and has patented its matching model.\\n\\n## Recent News and Announcements\\n\\n*   May 28, 2025: \"Connecting my courses – become an LLM expert and leader\"\\n*   May 18, 2025: \"2025 AI Executive Briefing\"\\n*   April 21, 2025: \"The Complete Agentic AI Engineering Course\"\\n*   January 23, 2025: \"LLM Workshop – Hands-on with Agents – resources\"\\n\\n## Platforms and Contact Information\\n\\n*   LinkedIn\\n*   Twitter\\n*   Facebook\\n*   Subscribe to newsletter\\n*   [ed@edwarddonner.com](mailto:ed@edwarddonner.com)\\n*   www.edwarddonner.com'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bc0afe-40ff-4b75-8715-66e5851d68b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-833', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, so I need to figure out the definitions of a few key concepts behind Large Language Models (LLMs). The terms mentioned are neural networks, attention, and transformers. Let me break this down step by step.\\n\\nFirst, I know that an LLM is primarily made up of something called a \"neural network.\" But what exactly is this neural network? From what I\\'ve heard before, neural networks are designed to simulate the way the human brain works. They consist of layers of neurons, right? Each layer has units or nodes that receive inputs and produce some sort of activation signal.\\n\\nI think these neural networks are often deep and have multiple layers. Each layer could process different parts of the data differently. But how exactly do they work to make predictions or generate responses?\\n\\nWait, I remember something about these models having a lot of parameters—like billions. That probably means there\\'s a lot of processing power needed to train them effectively.\\n\\nNow, the second concept is attention. I\\'ve heard that attention plays a central role in understanding how models process information. What does attention really mean here? Maybe it refers to how the model determines what parts of its input are most relevant for generating an output. It\\'s like paying attention to specific keywords or events in a text.\\n\\nI think attention can be implemented in different ways, though. For example, some models might focus on local patterns within smaller chunks of the data, while others use more global awareness across the entire dataset. This contextual sensitivity must be crucial for generating coherent responses from an LLM.\\n\\nNext up is the transformer architecture itself. Transformers seem like a recent big advancement in machine learning with respect to language models. They usually involve multiple layers where each layer processes information through self-attention mechanisms.\\n\\nWhat are these self-attention mechanisms? I think they allow each model unit to compute its own attention, which captures relationships between different parts of the input data. This could help in understanding dependencies and context within sequences.\\n\\nI also recall something about frequency analysis. Do transformers use frequency-based representations? That might include embeddings for words and their contexts. Using these global features could enhance the model\\'s ability to understand sentence structures and other linguistic patterns.\\n\\nSo, putting it all together: LLMs are neural networks with multiple layers that process information using attention mechanisms within a transformer framework. These systems learn from data by paying attention to certain parts of the input and generating outputs based on global context gained through attention and self-attention processes.\\n</think>\\n\\nLarge Language Models (LLMs), such as GPT, are composed of several key components that work together to generate human-like responses from text. Here\\'s a structured explanation of their underlying concepts:\\n\\n1. **Neural Network Architecture**:\\n   - A neural network is modeled after the structure of neurons in the brain, consisting of layers with interconnected nodes (units).\\n   - Each layer processes data differently and often involves multiple hidden layers.\\n   - These networks are typically deep and capable of processing large amounts of information simultaneously.\\n\\n2. **Attention Mechanism**:\\n   - Attention determines the relevance of parts of the input or model\\'s knowledge when generating an output.\\n   - For LLMs, it identifies the most relevant words or events in a text document to produce coherent responses.\\n   - This local focus aids in understanding context and generating more accurate answers.\\n\\n3. **Transformer Architecture**:\\n   - Transformer models use multiple layers with self-attention mechanisms.\\n   - In each layer, units process information based on their own attention, capturing relationships between different data parts.\\n   - These layers enable the model to understand sentence structure by incorporating global features from embeddings of words and their contexts.\\n\\n4. **Global Context**:\\n   - Transformers employ frequency-based representations, including embeddings for context and word embedding vectors.\\n   - This helps in understanding linguistic patterns beyond local context, enhancing the model\\'s ability to produce nuanced responses.\\n\\nIn summary, while LLMs consist of neural networks with multiple layers and attention mechanisms (specifically self-attention), transformers employ even more sophisticated structure and layer operations. These components work together to enable models to understand language by paying attention to relevant parts and leveraging global contextual features.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1751405831, model='deepseek-r1:1.5b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=837, prompt_tokens=23, total_tokens=860, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c1086b-47f9-4985-b724-1cac4e395e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a15e305-ad84-47e5-8b94-33c3f57a9a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Website Summary\n",
       "\n",
       "**About the Founder**\n",
       "Ed is the co-founder and CTO of Nebula.io, an AI company applying AI to help people discover their potential. He has previously founded and CEO'd AI startup untapt, acquired in 2021.\n",
       "\n",
       "### News and Announcements\n",
       "\n",
       "* **May 28, 2025**: Connecting my courses – become an LLM expert and leader\n",
       "* **May 18, 2025**: 2025 AI Executive Briefing\n",
       "* **April 21, 2025**: The Complete Agentic AI Engineering Course\n",
       "* **January 23, 2025**: LLM Workshop – Hands-on with Agents – resources"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef31fad",
   "metadata": {},
   "source": [
    "# from week 2: use gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d35e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://edwarddonner.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=summarize, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10529716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=summarize,\n",
    "    inputs=[gr.Textbox(label=\"url:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c89c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
